{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Squad Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"deepset/gbert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset = load_dataset('squad', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_context(dataset):\n",
    "    contexts, ctx2title = set(), {}\n",
    "    for row in dataset:\n",
    "        title, ctx = row['title'], row['context']\n",
    "        contexts.add(ctx)\n",
    "        ctx2title[ctx] = title\n",
    "    return list(contexts), ctx2title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts, ctx2title = get_unique_context(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genes are regulated so that they are expressed only when the product is needed, since expression draws on limited resources.:7 A cell regulates its gene expression depending on its external environment (e.g. available nutrients, temperature and other stresses), its internal environment (e.g. cell division cycle, metabolism, infection status), and its specific role if in a multicellular organism. Gene expression can be regulated at any step: from transcriptional initiation, to RNA processing, to post-translational modification of the protein. The regulation of lactose metabolism genes in E. coli (lac operon) was the first such mechanism to be described in 1961. \n",
      " Gene\n"
     ]
    }
   ],
   "source": [
    "ctx = contexts[11]\n",
    "print(ctx, '\\n', ctx2title[ctx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102, 20466, 30886, 5168, 10191, 3964, 262, 6711, 20294, 5168]\n",
      "[CLS] Genes are regulated so that they are\n"
     ]
    }
   ],
   "source": [
    "enc = tokenizer(ctx)['input_ids']\n",
    "print(enc[0:10])\n",
    "print(tokenizer.decode(enc[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def encode_contexts(contexts):\n",
    "    enc_ctx = []\n",
    "    for ctx in tqdm(contexts):\n",
    "        enc_ctx.append(tokenizer.encode(ctx, max_length=2048, truncation=True))\n",
    "    return enc_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18891/18891 [00:06<00:00, 2813.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1236"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_contexts = encode_contexts(contexts)\n",
    "max( list( len(e) for e in encoded_contexts ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Americans with Sub-Saharan African ancestry for historical reasons: slavery, partus sequitur ventrem, one-eighth law, the one-drop rule of 20th-century legislation, have frequently been classified as black (historically) or African American, even if they have significant European American or Native American ancestry. As slavery became a racial caste, those who were enslaved and others of any African ancestry were classified by what is termed \"hypodescent\" according to the lower status ethnic group. Many of majority European ancestry and appearance \"married white\" and assimilated into white society for its social and economic advantages, such as generations of families identified as Melungeons, now generally classified as white but demonstrated genetically to be of European and sub-Saharan African ancestry.\n",
      "[CLS] Americans with Sub - Saharan African ancestry for historical reasons : slavery, partus sequitur ventrem, one - eighth law, the one - drop rule of 20th - century legislation, have frequently been classified as black ( historically ) or African American, even if they have significant European American or Native American ancestry. As slavery became a racial caste, those who were enslaved and others of any African ancestry were classified by what is termed \" hypodescent \" according to the lower status ethnic group. Many of majority European ancestry and appearance \" married white \" and assimilated into white society for its social and economic advantages, such as generations of families identified as Melungeons, now generally classified as white but demonstrated genetically to be of European and sub - Saharan African ancestry. [SEP]\n"
     ]
    }
   ],
   "source": [
    "def test_encode_decode(idx, encoded_contexts):\n",
    "    print(contexts[idx])\n",
    "    print(tokenizer.decode(encoded_contexts[idx]))\n",
    "\n",
    "test_encode_decode(50, encoded_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id2word(encoded_contexts):\n",
    "    id2word = {}\n",
    "    for enc_ctx in encoded_contexts:\n",
    "        for enc_id in enc_ctx:\n",
    "            if enc_id not in id2word:\n",
    "                id2word[enc_id] = tokenizer.decode(enc_id)\n",
    "    return id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = get_id2word(encoded_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_enc_context(encoded_contexts, id2word):\n",
    "    tkn_context = []\n",
    "    for enc_context in tqdm(encoded_contexts):\n",
    "        tkns = []\n",
    "        for enc_id in enc_context:\n",
    "            tkns.append(id2word[enc_id])\n",
    "        tkn_context.append(tkns)\n",
    "    return tkn_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18891/18891 [00:00<00:00, 61803.77it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenzied_context = decode_enc_context(encoded_contexts, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Bern',\n",
       " 'has',\n",
       " 'a',\n",
       " 'popul',\n",
       " '##ation',\n",
       " 'of',\n",
       " '140',\n",
       " ',',\n",
       " '63',\n",
       " '##4',\n",
       " 'people',\n",
       " 'and',\n",
       " '34',\n",
       " '%',\n",
       " 'of',\n",
       " 'the',\n",
       " 'popul',\n",
       " '##ation',\n",
       " 'are',\n",
       " 'res',\n",
       " '##ident',\n",
       " 'for',\n",
       " '##eign',\n",
       " 'national',\n",
       " '##s',\n",
       " '.',\n",
       " 'Over',\n",
       " 'the',\n",
       " '10',\n",
       " 'years',\n",
       " 'between',\n",
       " '2000',\n",
       " 'and',\n",
       " '2010',\n",
       " ',',\n",
       " 'the',\n",
       " 'popul',\n",
       " '##ation',\n",
       " 'ch',\n",
       " '##ange',\n",
       " '##d',\n",
       " 'at',\n",
       " 'a',\n",
       " 'rat',\n",
       " '##e',\n",
       " 'of',\n",
       " '0',\n",
       " '.',\n",
       " '6',\n",
       " '%',\n",
       " '.',\n",
       " 'Migration',\n",
       " 'acc',\n",
       " '##ou',\n",
       " '##nte',\n",
       " '##d',\n",
       " 'for',\n",
       " '1',\n",
       " '.',\n",
       " '3',\n",
       " '%',\n",
       " ',',\n",
       " 'wh',\n",
       " '##ile',\n",
       " 'bi',\n",
       " '##rt',\n",
       " '##hs',\n",
       " 'and',\n",
       " 'de',\n",
       " '##ath',\n",
       " '##s',\n",
       " 'acc',\n",
       " '##ou',\n",
       " '##nte',\n",
       " '##d',\n",
       " 'for',\n",
       " '−',\n",
       " '##2',\n",
       " '.',\n",
       " '1',\n",
       " '%',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenzied_context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_tokenize(contexts):\n",
    "    return [ctx.split(\" \") for ctx in contexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_tokenized_context = naive_tokenize(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes = id_vocab\n",
    "# G.add_nodes_from(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (node_a) --[context_id, ...]--> (node_b)\n",
    "def create_graph(tokenzied_context):\n",
    "    G = nx.Graph()\n",
    "    for ctx_idx, context in tqdm(enumerate(tokenzied_context), total=len(tokenzied_context)):\n",
    "        for i in range(len(context)-1):\n",
    "            node_a, node_b = context[i], context[i+1] \n",
    "            G.add_nodes_from([node_a, node_b])\n",
    "            if G.has_edge(node_a, node_b):\n",
    "                G[node_a][node_b]['ctx_idx'].append(ctx_idx)\n",
    "            else:\n",
    "                G.add_edge(node_a, node_b, ctx_idx=[ctx_idx])\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18891/18891 [00:05<00:00, 3402.47it/s]\n"
     ]
    }
   ],
   "source": [
    "nG = create_graph(naive_tokenized_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "her",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
